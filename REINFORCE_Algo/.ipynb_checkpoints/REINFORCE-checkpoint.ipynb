{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495af41c",
   "metadata": {},
   "source": [
    "# REINFORCE Algothm from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0a00f",
   "metadata": {},
   "source": [
    "The task we are trying to solve is: if you get placed into a random cell within an N by N grid. What is the quickest way to get out of the grid (go to the exit). The exit is fixed and is always in the top right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1b1d1",
   "metadata": {},
   "source": [
    "Add the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975d7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d987aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b1f96",
   "metadata": {},
   "source": [
    "Set up a simple class to give the grid an interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa829b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, n, m, exit_pos, figure_pos):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.exit_pos = exit_pos\n",
    "        self.figure_pos = figure_pos\n",
    "\n",
    "    def move(self, direction):\n",
    "        x, y = self.figure_pos\n",
    "        if direction == \"up\":\n",
    "            if y < self.n-1:\n",
    "                self.figure_pos = (x, y+1)\n",
    "        elif direction == \"down\":\n",
    "            if y > 0:\n",
    "                self.figure_pos = (x, y-1)\n",
    "        elif direction == \"left\":\n",
    "            if x > 0:\n",
    "                self.figure_pos = (x-1, y)\n",
    "        elif direction == \"right\":\n",
    "            if x < self.m-1:\n",
    "                self.figure_pos = (x+1, y)\n",
    "\n",
    "    def is_at_exit(self):\n",
    "        return self.figure_pos == self.exit_pos\n",
    "\n",
    "    def get_state(self, device):\n",
    "        return torch.FloatTensor(self.figure_pos).unsqueeze(0).to(device)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352451e",
   "metadata": {},
   "source": [
    "The first input of the REINFORCE is a policy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028c1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.fc2 = nn.Linear(16, 4) # four outputs for the 4 actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.softmax(x, dim=1)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0961f81",
   "metadata": {},
   "source": [
    "REINFORCE assumed the episodic task setting. Fortunately for us, in this case the task is clearly episodic: the episode ends whenever the agent finds the exit. In a grid that is small, it happens fairly quickly. We therefore need a function that is able to generate the whole episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748ac124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(grid, policy_net, device=\"cpu\", max_episode_len = 100):\n",
    "    state = grid.get_state(device)\n",
    "    ep_length = 0\n",
    "    while not grid.is_at_exit():\n",
    "        # Convert state to tensor and pass through policy network to get action probabilities\n",
    "        ep_length+=1\n",
    "        action_probs = policy_net(state).squeeze()\n",
    "        log_probs = torch.log(action_probs)\n",
    "        cpu_action_probs = action_probs.detach().cpu().numpy()\n",
    "        action = np.random.choice(np.arange(4), p=cpu_action_probs)\n",
    "\n",
    "        # Take the action and get the new state and reward\n",
    "        grid.move(actions[action])\n",
    "        next_state = grid.get_state(device)\n",
    "        reward = -0.1 if not grid.is_at_exit() else 0\n",
    "\n",
    "        # Add the state, action, and reward to the episode\n",
    "        new_episode_sample = (state, action, reward)\n",
    "        yield new_episode_sample, log_probs\n",
    "\n",
    "        # We do not want to add the state, action, and reward for reaching the exit position\n",
    "        if reward == 0:\n",
    "            break\n",
    "\n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "        if ep_length > max_episode_len:\n",
    "            return\n",
    "\n",
    "    # Add the final state, action, and reward for reaching the exit position\n",
    "    new_episode_sample = (grid.get_state(device), None, 0)\n",
    "    yield new_episode_sample, log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d01b2c",
   "metadata": {},
   "source": [
    "Remember, policy basically describes probabilities of taking certain actions under certain conditions. Also, remember the REINFORCE update rule needs the gradient of the log of the probability, hence we calculate logs of probabilities here.\n",
    "\n",
    "We will also need a few helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae8f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_wrt_params(\n",
    "    net: torch.nn.Module, loss_tensor: torch.Tensor\n",
    "):\n",
    "    # Dictionary to store gradients for each parameter\n",
    "    # Compute gradients with respect to each parameter\n",
    "    for name, param in net.named_parameters():\n",
    "        g = grad(loss_tensor, param, retain_graph=True)[0]\n",
    "        param.grad = g\n",
    "\n",
    "def update_params(net: torch.nn.Module, lr: float) -> None:\n",
    "    # Update parameters for the network\n",
    "    for name, param in net.named_parameters():\n",
    "        param.data += lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730197fe",
   "metadata": {},
   "source": [
    "Then finally code for the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2eb6aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_good_starting_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m all_iterations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m all_log_probs \u001b[38;5;241m=\u001b[39m []    \n\u001b[1;32m---> 17\u001b[0m grid \u001b[38;5;241m=\u001b[39m get_good_starting_grid()\n\u001b[0;32m     18\u001b[0m episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(generate_episode(grid, policy_net\u001b[38;5;241m=\u001b[39mpolicy_net, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m     19\u001b[0m lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(episode))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_good_starting_grid' is not defined"
     ]
    }
   ],
   "source": [
    "policy_net = PolicyNet()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy_net.to(device)\n",
    "\n",
    "lengths = []\n",
    "rewards = []\n",
    "\n",
    "gamma = 0.99\n",
    "lr_policy_net = 2**-13\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr_policy_net)\n",
    "\n",
    "prefix = \"reinforce-per-step\"\n",
    "\n",
    "for episode_num in tqdm(range(2500)):\n",
    "    all_iterations = []\n",
    "    all_log_probs = []    \n",
    "    grid = get_good_starting_grid()\n",
    "    episode = list(generate_episode(grid, policy_net=policy_net, device=device))\n",
    "    lengths.append(len(episode))\n",
    "    loss = 0\n",
    "    for t, ((state, action, reward), log_probs) in enumerate(episode[:-1]):\n",
    "        gammas_vec = gamma ** (torch.arange(t+1, len(episode))-t-1)\n",
    "        # Since the reward is -1 for all steps except the last, we can just sum the gammas\n",
    "        G = - torch.sum(gammas_vec)\n",
    "        rewards.append(G.item())\n",
    "        policy_loss = log_probs[action]\n",
    "        optimizer.zero_grad()\n",
    "        gradients_wrt_params(policy_net, policy_loss)\n",
    "        update_params(policy_net, lr_policy_net  * G * gamma**t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615cdbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
